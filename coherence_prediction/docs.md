# Unsupervised Predictive Coherence Assessment  

## Overview  
The **Unsupervised Predictive Coherence Assessment** approach trains a model to determine whether a given text maintains coherence by evaluating how well a missing portion aligns with its surrounding context. Instead of manually labeling texts as coherent or incoherent, we leverage **Next Sentence Prediction (NSP)** and **synthetic incoherence generation** to create a fully unsupervised framework.  

To enhance model robustness, we introduce a **corruption model** that automatically generates plausible yet incoherent text samples. This ensures the model learns to distinguish natural coherence from subtle but realistic disruptions.  

## Problem Statement  
Given a short span of text (10-20 tokens), can a model predict whether a continuation maintains coherence? Instead of classifying entire passages, we evaluate **predictive coherence**: can the missing portion be meaningfully completed while ensuring a smooth flow in the overall passage?  

To achieve this, we need a dataset with both:  
1. **Coherent sequences**: Naturally occurring text snippets.  
2. **Incoherent sequences**: Synthetic disruptions generated by an **automated corruption model** instead of simple random word swaps.  

## Approach  

### 1. **Dataset Creation**  
- **Source Texts**: Extract text passages from **NLTK’s corpora** to ensure diversity across genres.  
- **Segment Extraction**: Select coherent spans of **10-20 tokens** for training.  
- **Negative Sample Generation (Corruption Model)**: Instead of random replacements, we generate **intelligently disrupted** text via:  
  - **Light Transformer Paraphrasing:** A small seq2seq model (T5-small, DistilGPT-2) trained to produce slightly incorrect but plausible text.  
  - **Rule-Based Mutations:**  
    - **Syntax breaking:** `"The qu ick, brown fox, jumpAd ov er the lazy dog."`  
    - **Semantic drift:** `"The scientist observed the ocean under the telescope."`  
    - **Tense shifts:** `"He runs to the store and was buying milk yesterday."`  
    - **Nonsense inserts:** `"The funeral will be held in 1977 at heaven."`  
    - **Reordered structures:** `"Once upon a time in America’s heartlands, the story was ended before it began."`  

### 2. **Model Architecture**  
- **Pretrained Transformer (BERT, RoBERTa) with NSP-style Training**  
- Input: `(previous tokens, candidate continuation)`  
- Output: Binary classification (1 = coherent, 0 = incoherent)  
- **Training Modifications**:  
  - Instead of full sentences, train on **10-20 token segments**.  
  - Use **contrastive learning**: The model must rank real continuations higher than synthetic ones.  

### 3. **Training Process**  
- **Positive Examples**: Naturally occurring sequences.  
- **Negative Examples**: Corrupted sequences from the paraphrase + rule-based model.  
- **Loss Function**: Binary cross-entropy for coherence classification.  

### 4. **Evaluation Metrics**  
- **Binary Accuracy**: Can the model classify coherent vs. incoherent segments?  
- **Self-Consistency**: Does it make stable predictions across reworded inputs?  
- **Perplexity**: Lower perplexity for coherent text, higher for corrupted text.  

### 5. **Fine-Tuning**  
- Train on mixed genres to improve generalization.  
- Domain-specific fine-tuning for applications in **text generation, discourse analysis, and automated grading**.  

## Benefits  
- **Fully Unsupervised**: No human annotation required.  
- **More Robust Coherence Detection**: Unlike Masked Language Modeling (MLM), this method explicitly evaluates **predictive coherence**.  
- **Applicable to Real-World NLP Tasks**:  
  - Improving **autocomplete systems**  
  - Evaluating **AI-generated text quality**  
  - Enhancing **fake news detection & misinformation filtering**  

## Conclusion  
By combining **Next Sentence Prediction (NSP)** with a **custom corruption model**, this approach provides a scalable method for assessing coherence in short text spans. Training on **NLTK-based datasets**, the model learns to distinguish fluent from disrupted text, enabling applications in **automated text evaluation, AI-assisted writing tools, and content validation**.  
